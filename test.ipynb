{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAY BABA PANCHBADAN\n"
     ]
    }
   ],
   "source": [
    "print(\"JAY BABA PANCHBADAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = \"Markdown to PDF.pdf\"  # Replace with your file\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = extract_text_with_pdfplumber(\"Markdown to PDF.pdf\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "docx_file = \"ISJ Draft.docx\"  # Replace with your file\n",
    "text = extract_text_from_docx(docx_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = \"ISJ Draft.pdf\"  # Replace with your file\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi1\\AppData\\Local\\Temp\\ipykernel_17896\\1513378511.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(model_name=\"gpt-4o\", api_key=OPENAI_API_KEY),  # or \"gpt-4\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o\", api_key=OPENAI_API_KEY),  # or \"gpt-4\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi1\\AppData\\Local\\Temp\\ipykernel_17896\\2841421832.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Yes, Ravi Shankar is listed as one of the authors in the paper titled \"BARBARIK – A Decentralized Solution for Refugee Identity, Governance and Aid Transparency.\"\n",
      "\n",
      "Sources:\n",
      "- on narrow applications like remittances. \n",
      "In this research, we proposed a novel framework BARBARIK, a blockchain-AI \n",
      "ecosystem inspired by the Mahabharata’s myth of Barbarik, a warrior renowned for hi\n",
      "- BARBARIK – A Decentralized Solution for \n",
      "Refugee Identity, Governance and Aid \n",
      "Transparency \n",
      "Hemraj Shobharam Lamkuche1*†, Rahul B. Hiremath2‡, \n",
      "Ravi Shankar1 and Matrupriya Dibyanshu Panda1 \n",
      "1School \n",
      "- poses BARBARIK, a blockchain based decentralized identity management framework \n",
      "inspired by Mahabharata’s myth of Barbarik, renowned for his impartiality and strategic \n",
      "precision. The framework integr\n",
      "- Smart Healthcare Solution for Secure and Privacy-Preserving Data Access. IEEE Systems Journal, 16(3), \n",
      "3746–3757. https://doi.org/10.1109/JSYST.2021.3092519 \n",
      "21\n",
      " \n",
      "ZEDURI, M., STANCANELLI, E., BONACCOR\n"
     ]
    }
   ],
   "source": [
    "query = \"is there any Ravi Shankar in the paper?\"  # Replace with any user query\n",
    "response = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:200])  # print preview of sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"can u tell me the methodology used ?\"  # Replace with any user query\n",
    "response = qa_chain(query)\n",
    "\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:200])  # print preview of sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY),\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"list all the authors of the paper\"\n",
    "response = qa_chain(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The context provides author information from several papers, but does not specify a single paper. Here are the authors from the mentioned papers:\n",
      "\n",
      "1. Diro, A., Zhou, L., Saini, A., Kaisar, S., & Hiep, P. C. \n",
      "2. Djamali, A., Dossow, P., Hinterstocker, M., Schellinger, B., Sedlmeir, J., Völter, F., & Willburger, L.\n",
      "3. Francia, A., Mariani, S., Adduce, G., Vecchiarelli, S., & Zambonelli, F.\n",
      "4. Godden, T., Smet, R. D., Debruyne, C., Vandervelden, T., Steenhaut, K., & Braeken, A.\n",
      "5. ZEDURI, M., STANCANELLI, E., BONACCORSI, G., ODONE, A., & GORINI, G.\n",
      "6. Zeydan, E., Mangues, J., Arslan, S. S., & Turk, Y.\n",
      "\n",
      "If you meant a specific paper, please clarify which one.\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Tokenize chunks\n",
    "tokenized_chunks = [chunk.split() for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "# Save chunks with metadata\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": f\"chunk_{i}\"}) for i, chunk in enumerate(chunks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi1\\AppData\\Local\\Temp\\ipykernel_17896\\2308886246.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieval(query, top_k=5):\n",
    "    # BM25\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "    bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
    "    bm25_docs = [documents[i] for i in bm25_top_indices]\n",
    "    \n",
    "    # FAISS\n",
    "    faiss_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Merge and deduplicate (based on content)\n",
    "    seen = set()\n",
    "    hybrid_docs = []\n",
    "    for doc in bm25_docs + faiss_docs:\n",
    "        if doc.page_content not in seen:\n",
    "            hybrid_docs.append(doc)\n",
    "            seen.add(doc.page_content)\n",
    "    \n",
    "    return hybrid_docs[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", api_key = OPENAI_API_KEY)\n",
    "\n",
    "def flash_rag_bm25_faiss(query):\n",
    "    context_docs = hybrid_retrieval(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"Use the context below to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi1\\AppData\\Local\\Temp\\ipykernel_17896\\789967213.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  faiss_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The context provided does not contain information about \"Ravi Shankar.\" Based on common knowledge, Ravi Shankar was a renowned Indian sitar virtuoso and composer, known for his profound influence on the world of music, particularly in bringing Indian classical music to the Western world. If this is not the Ravi Shankar you are referring to, please provide additional context or specify the relevant field or context related to Ravi Shankar.\n",
      "Answer: The tech stack proposed in the context includes blockchain, Zero-Knowledge Proofs (ZKPs), and Decentralized Identifiers (DIDs). These technologies are integrated into a decentralized identity management framework called BARBARIK, designed to empower refugees with self-sovereign digital identities, enhance privacy, and enable cross-border recognition. The use of blockchain is central to ensuring transparency, security, and compliance with international law, while ZKPs and DIDs contribute to maintaining privacy and identity verification.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_q = input(\"Ask a question (or 'exit'): \")\n",
    "    if user_q.lower() == 'exit':\n",
    "        break\n",
    "    answer = flash_rag_bm25_faiss(user_q)\n",
    "    print(\"Answer:\", answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The context provided does not specifically mention \"Seria\" in relation to refugees. However, it discusses a framework called BARBARIK that uses blockchain and artificial intelligence to enhance the management of refugee identities and resources. The system focuses on providing refugees with control over their personal information and ensuring secure and equitable access to services through decentralized identity systems and verifiable credentials. It also addresses challenges such as regulatory compliance, data privacy, and algorithmic bias in aid distribution. If \"Seria\" is meant to refer to a specific topic or region related to refugees, it is not covered in this context.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_q = input(\"Ask a question (or 'exit'): \")\n",
    "    if user_q.lower() == 'exit':\n",
    "        break\n",
    "    answer = flash_rag_bm25_faiss(user_q)\n",
    "    print(\"Answer:\", answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting agno\n",
      "  Downloading agno-1.2.11-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting docstring-parser (from agno)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gitpython (from agno)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (2.8.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (2.11.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (1.1.0)\n",
      "Collecting python-multipart (from agno)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (6.0.2)\n",
      "Requirement already satisfied: rich in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (13.9.4)\n",
      "Collecting tomli (from agno)\n",
      "  Using cached tomli-2.2.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typer in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from agno) (4.12.2)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->agno)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from httpx->agno) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from httpx->agno) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from httpx->agno) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from httpx->agno) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from httpcore==1.*->httpx->agno) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from pydantic->agno) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from pydantic->agno) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from pydantic->agno) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from rich->agno) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from rich->agno) (2.15.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from typer->agno) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from typer->agno) (1.5.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from click>=8.0.0->typer->agno) (0.4.6)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->agno)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->agno) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from anyio->httpx->agno) (1.3.1)\n",
      "Downloading agno-1.2.11-py3-none-any.whl (598 kB)\n",
      "   ---------------------------------------- 0.0/598.9 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 524.3/598.9 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 598.9/598.9 kB 4.3 MB/s eta 0:00:00\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached tomli-2.2.1-cp311-cp311-win_amd64.whl (108 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tomli, smmap, python-multipart, docstring-parser, gitdb, gitpython, agno\n",
      "Successfully installed agno-1.2.11 docstring-parser-0.16 gitdb-4.0.12 gitpython-3.1.44 python-multipart-0.0.20 smmap-5.0.2 tomli-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install agno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Using cached pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (2.0.40)\n",
      "Collecting pgvector\n",
      "  Downloading pgvector-0.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting psycopg[binary]\n",
      "  Downloading psycopg-3.2.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from psycopg[binary]) (2025.2)\n",
      "Collecting psycopg-binary==3.2.6 (from psycopg[binary])\n",
      "  Downloading psycopg_binary-3.2.6-cp311-cp311-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ravi1\\anaconda3\\envs\\nlp1\\lib\\site-packages (from pgvector) (1.26.4)\n",
      "Downloading psycopg_binary-3.2.6-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading pgvector-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading psycopg-3.2.6-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: psycopg-binary, psycopg, pgvector\n",
      "Successfully installed pgvector-0.4.0 psycopg-3.2.6 psycopg-binary-3.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy psycopg[binary] pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`pgvector` not installed. Please install using `pip install pgvector`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ravi1\\anaconda3\\envs\\nlp1\\Lib\\site-packages\\agno\\vectordb\\pgvector\\pgvector.py:17\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpgvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vector\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pgvector'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknowledge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PDFKnowledgeBase, PDFReader\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectordb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgvector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PgVector\n\u001b[0;32m      4\u001b[0m pdf_knowledge_base \u001b[38;5;241m=\u001b[39m PDFKnowledgeBase(\n\u001b[0;32m      5\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISJ Draft.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Table name: ai.pdf_documents\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     reader\u001b[38;5;241m=\u001b[39mPDFReader(chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ravi1\\anaconda3\\envs\\nlp1\\Lib\\site-packages\\agno\\vectordb\\pgvector\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectordb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distance\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectordb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HNSW, Ivfflat\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectordb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpgvector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PgVector\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectordb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SearchType\n",
      "File \u001b[1;32mc:\\Users\\ravi1\\anaconda3\\envs\\nlp1\\Lib\\site-packages\\agno\\vectordb\\pgvector\\pgvector.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpgvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vector\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pgvector` not installed. Please install using `pip install pgvector`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magno\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedder\n",
      "\u001b[1;31mImportError\u001b[0m: `pgvector` not installed. Please install using `pip install pgvector`"
     ]
    }
   ],
   "source": [
    "from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader\n",
    "from agno.vectordb.pgvector import PgVector\n",
    "\n",
    "pdf_knowledge_base = PDFKnowledgeBase(\n",
    "    path=\"ISJ Draft.pdf\",\n",
    "    # Table name: ai.pdf_documents\n",
    "    vector_db=PgVector(\n",
    "        table_name=\"pdf_documents\",\n",
    "        db_url=\"postgresql+psycopg://ai:ai@localhost:5532/ai\",\n",
    "    ),\n",
    "    reader=PDFReader(chunk=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from knowledge_base import knowledge_base\n",
    "\n",
    "agent = Agent(\n",
    "    knowledge=knowledge_base,\n",
    "    search_knowledge=True,\n",
    ")\n",
    "agent.knowledge.load(recreate=False)\n",
    "\n",
    "agent.print_response(\"Ask me about something from the knowledge base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
